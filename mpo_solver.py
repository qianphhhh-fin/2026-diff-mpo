"""
è„šæœ¬åç§°: mpo_solver.py
åŠŸèƒ½æè¿°: 
    å®ç°å¯å¾®å¤šå‘¨æœŸä¼˜åŒ– (Differentiable MPO) çš„æ ¸å¿ƒæ±‚è§£å™¨ã€‚
    é€šè¿‡è‡ªå®šä¹‰ PyTorch Autograd Functionï¼Œå®ç°äº†å‰å‘æ±‚è§£ (Forward) å’Œåå‘ä¼ æ’­ (Backward)ã€‚

ä¸»è¦ç»„ä»¶:
    1. DifferentiableMPO (nn.Module): 
       - å°è£…äº†æ±‚è§£å™¨çš„æ¥å£ã€‚
       - solve_forward_md: å®ç°åŸºäºé•œåƒä¸‹é™ (Mirror Descent) çš„å¿«é€Ÿå‰å‘æ±‚è§£å™¨ï¼Œæ”¯æŒ GPU åŠ é€Ÿã€‚
    2. MDFPIdentity (autograd.Function):
       - forward: è°ƒç”¨ solve_forward_md è®¡ç®—æœ€ä¼˜æƒé‡ w*ã€‚
       - backward: ä½¿ç”¨éšå‡½æ•°å®šç† (Implicit Function Theorem) å’Œ Neumann çº§æ•°è¿‘ä¼¼ï¼Œ
         é«˜æ•ˆè®¡ç®— Loss å¯¹è¾“å…¥å‚æ•° (mu, L, w_prev) çš„æ¢¯åº¦ã€‚

è¾“å…¥:
    - mu: é¢„æµ‹æ”¶ç›Šç‡ã€‚
    - L: é¢„æµ‹åæ–¹å·®çš„ Cholesky å› å­ã€‚
    - w_prev: åˆå§‹æŒä»“ã€‚
    - cvar_limit: é£é™©çº¦æŸä¸Šé™ã€‚

è¾“å‡º:
    - w_star: æœ€ä¼˜æŠ•èµ„ç»„åˆæƒé‡ï¼Œå¸¦æœ‰æ¢¯åº¦ä¿¡æ¯ã€‚

ä¸å…¶ä»–è„šæœ¬çš„å…³ç³»:
    - è¢« model.py è°ƒç”¨ï¼Œä½œä¸ºç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚ (Optimization Layer)ã€‚
    - ä¾èµ– config.py è·å–ä¼˜åŒ–å‚æ•° (Gamma, Cost Coeff, CVaR Penalty)ã€‚
"""

import torch
import torch.nn as nn
from scipy.stats import norm
from config import cfg

class MDFPIdentity(torch.autograd.Function):
    @staticmethod
    def forward(ctx, mu, L, w_prev, cvar_limit, w_star, H, N, cfg_dict):
        ctx.save_for_backward(mu, L, w_prev, cvar_limit, w_star)
        ctx.cfg = cfg_dict
        ctx.H = H
        ctx.N = N
        return w_star

    @staticmethod
    def backward(ctx, grad_output):
        # 2. Backward Pass: MDFP
        # Vectorized implementation
        with torch.enable_grad():
            mu, L, w_prev, cvar_limit, w_star = ctx.saved_tensors
            cfg_dict = ctx.cfg
            H, N = ctx.H, ctx.N
            
            eta = 1e-1 
            B_iter = 5     # Reduced iterations for speed, usually sufficient
            
            gamma = cfg_dict['gamma']
            cost_coeff = cfg_dict['cost_coeff']
            kappa = cfg_dict['kappa']
            cvar_penalty = cfg_dict.get('cvar_penalty', 50.0) # ä½¿ç”¨ Config é…ç½®
            
            # --- A. Define Objective Gradient \nabla_w F(w) ---
            w = w_star.detach().clone().requires_grad_(True)
            
            # Vectorized Objective Calculation
            # 1. Return: - mu^T w
            loss_ret = - (mu * w).sum()
            
            # 2. Risk: || L^T w ||^2
            # L: (B, H, N, N), w: (B, H, N) -> (B, H, N, 1)
            # matmul: (B, H, N, N).mT @ (B, H, N, 1) -> (B, H, N, 1)
            L_T_w = torch.matmul(L.transpose(-1, -2), w.unsqueeze(-1))
            loss_risk = (L_T_w.squeeze(-1) ** 2).sum()
            
            # 3. Cost: smoothed L1 of (w_t - w_{t-1})
            # Prepend w_prev to w along time dimension
            # w_prev: (B, N) -> (B, 1, N)
            w_shifted = torch.cat([w_prev.unsqueeze(1), w[:, :-1, :]], dim=1)
            diff = w - w_shifted
            loss_cost = torch.sum(torch.sqrt(diff**2 + 1e-8))
            
            # 4. CVaR Penalty: cvar_penalty * Softplus(-mu_p + kappa*sigma_p - limit)
            if cvar_penalty > 1e-6:
                mu_p = (mu * w).sum(dim=-1) # (B, H)
                sigma_p = torch.norm(L_T_w.squeeze(-1), p=2, dim=-1) # (B, H)
                
                limit_val = cvar_limit if cvar_limit.dim() > 0 else cvar_limit.unsqueeze(0)
                # Broadcast limit_val to (B, H) if necessary
                if limit_val.dim() == 1:
                    limit_val = limit_val.unsqueeze(1)
                
                violation = -mu_p + kappa * sigma_p - limit_val
                # Softplus approximation of ReLU for smoothness
                loss_cvar = cvar_penalty * torch.nn.functional.softplus(violation, beta=50).sum()
            else:
                loss_cvar = 0.0
            
            F = loss_ret + gamma * loss_risk + cost_coeff * loss_cost + loss_cvar
            
            if not F.requires_grad:
                return (None,) * 8
            
            grad_F, = torch.autograd.grad(F, w, create_graph=True)
            
            # --- B. Neumann Series for (I - J)^-1 ---
            # J^T v = (v - <z*, v>1) - eta * HVP(z* * (v - <z*, v>1))
            
            curr_v = grad_output.clone()
            sum_v = grad_output.clone()
            
            # Vectorized Neumann Loop
            for k in range(B_iter):
                # 1. Projection: v_temp = v - <v, w*> 1
                # w_star: (B, H, N), curr_v: (B, H, N)
                dot_val = (curr_v * w_star).sum(dim=-1, keepdim=True) # (B, H, 1)
                v_temp = curr_v - dot_val # Broadcasting subtract
                
                # 2. Hessian Vector Product
                vec = v_temp * w_star
                
                # Efficient HVP using autograd
                grad_F_dot_vec = torch.sum(grad_F * vec)
                # retain_graph=True is needed because we differentiate grad_F multiple times
                H_vec, = torch.autograd.grad(grad_F_dot_vec, w, retain_graph=True)
                
                next_v = v_temp - eta * H_vec
                
                curr_v = next_v
                sum_v = sum_v + curr_v
            
            # --- C. Parameter Gradients ---
            # u_P = sum_v - <sum_v, w*> 1
            dot_val_sum = (sum_v * w_star).sum(dim=-1, keepdim=True)
            u_P = sum_v - dot_val_sum
            
            u_hat = (u_P * w_star).detach()
            
            grad_F_dot_uhat = torch.sum(grad_F * u_hat)
            
            # Only compute gradients for inputs that require grad
            inputs_to_grad = []
            inputs_indices = []
            
            if mu.requires_grad: inputs_to_grad.append(mu); inputs_indices.append(0)
            if L.requires_grad: inputs_to_grad.append(L); inputs_indices.append(1)
            if w_prev.requires_grad: inputs_to_grad.append(w_prev); inputs_indices.append(2)
            if cvar_limit.requires_grad: inputs_to_grad.append(cvar_limit); inputs_indices.append(3)
            
            if len(inputs_to_grad) > 0:
                computed_grads = torch.autograd.grad(grad_F_dot_uhat, tuple(inputs_to_grad), retain_graph=False, allow_unused=True)
            else:
                computed_grads = []

            # Map back to full list
            d_mu, d_L, d_wprev, d_cvar = None, None, None, None
            
            # Helper to get grad from computed list
            curr_idx = 0
            if 0 in inputs_indices: d_mu = computed_grads[curr_idx]; curr_idx += 1
            if 1 in inputs_indices: d_L = computed_grads[curr_idx]; curr_idx += 1
            if 2 in inputs_indices: d_wprev = computed_grads[curr_idx]; curr_idx += 1
            if 3 in inputs_indices: d_cvar = computed_grads[curr_idx]; curr_idx += 1
            
            if d_mu is None: d_mu = torch.zeros_like(mu)
            if d_L is None: d_L = torch.zeros_like(L)
            if d_wprev is None: d_wprev = torch.zeros_like(w_prev)
            if d_cvar is None: d_cvar = torch.zeros_like(cvar_limit)
            
            return -eta * d_mu, -eta * d_L, -eta * d_wprev, -eta * d_cvar, None, None, None, None

class DifferentiableMPO(nn.Module):
    def __init__(self):
        super(DifferentiableMPO, self).__init__()
        self.H = cfg.PREDICT_HORIZON
        self.N = cfg.NUM_ASSETS
        self.cfg_dict = {
            'gamma': cfg.RISK_AVERSION,
            'cost_coeff': cfg.COST_COEFF,
            'kappa': norm.pdf(norm.ppf(cfg.CVAR_CONFIDENCE)) / (1 - cfg.CVAR_CONFIDENCE),
            'cvar_penalty': getattr(cfg, 'CVAR_PENALTY', 50.0) # [NEW]
        }
            
    def solve_forward_md(self, mu, L, w_prev, cvar_limit, max_iters=300, tol=1e-6):
        """
        Solve the forward problem using Mirror Descent (Entropic) on PyTorch.
        This avoids CvxpyLayer overhead and ensures the solution matches the backward pass objective.
        """
        B, H, N = mu.shape
        # Initialize w uniform
        w = torch.ones_like(mu) / N
        w.requires_grad_(False) # We don't track grad in forward solve
        
        eta = 0.05 # Tuned step size
        
        gamma = self.cfg_dict['gamma']
        cost_coeff = self.cfg_dict['cost_coeff']
        kappa = self.cfg_dict['kappa']
        cvar_penalty = self.cfg_dict.get('cvar_penalty', 50.0)
        
        for k in range(max_iters):
            # Compute Gradient of F w.r.t w
            # We can use autograd for convenience, but detach to avoid graph build up
            with torch.enable_grad():
                w_var = w.detach().requires_grad_(True)
                
                # Re-implement objective (same as backward)
                loss_ret = - (mu * w_var).sum()
                L_T_w = torch.matmul(L.transpose(-1, -2), w_var.unsqueeze(-1))
                loss_risk = (L_T_w.squeeze(-1) ** 2).sum()
                w_shifted = torch.cat([w_prev.unsqueeze(1), w_var[:, :-1, :]], dim=1)
                diff = w_var - w_shifted
                loss_cost = torch.sum(torch.sqrt(diff**2 + 1e-8))
                
                mu_p = (mu * w_var).sum(dim=-1)
                
                if cvar_penalty > 1e-6:
                    sigma_p = torch.norm(L_T_w.squeeze(-1), p=2, dim=-1)
                    limit_val = cvar_limit if cvar_limit.dim() > 0 else cvar_limit.unsqueeze(0)
                    if limit_val.dim() == 1: limit_val = limit_val.unsqueeze(1)
                    violation = -mu_p + kappa * sigma_p - limit_val
                    loss_cvar = cvar_penalty * torch.nn.functional.softplus(violation, beta=50).sum()
                else:
                    loss_cvar = 0.0
                
                F = loss_ret + gamma * loss_risk + cost_coeff * loss_cost + loss_cvar
                
                grad_F, = torch.autograd.grad(F, w_var)
            
            # Mirror Descent Step: w_{k+1} = w_k * exp(-eta * grad) / Norm
            # Log-space update for stability
            log_w = torch.log(w + 1e-10)
            log_w_new = log_w - eta * grad_F
            w_new = torch.softmax(log_w_new, dim=-1)
            
            # Check convergence
            dist = torch.norm(w_new - w)
            w = w_new
            if dist < tol:
                break
                
        return w

    def forward(self, mu, L, w_prev, cvar_limit=None):
        # å¦‚æœæœªæä¾› limitï¼Œä½¿ç”¨ Config é»˜è®¤å€¼
        if cvar_limit is None:
            # æ„é€ ä¸€ä¸ª scalar tensor
            cvar_limit = torch.tensor(cfg.CVAR_LIMIT, device=mu.device, dtype=mu.dtype)
        
        # ç¡®ä¿ cvar_limit æ˜¯ tensor ä¸”ç»´åº¦æ­£ç¡®
        if cvar_limit.dim() == 0:
            cvar_limit = cvar_limit.expand(mu.size(0)) # Expand to Batch Size
            
        # 1. Forward using custom MD solver (Fast)
        w_star_batch = self.solve_forward_md(mu, L, w_prev, cvar_limit)
            
        # 2. Attach MDFP Backward
        return MDFPIdentity.apply(mu, L, w_prev, cvar_limit, w_star_batch, self.H, self.N, self.cfg_dict)

# ==========================
# CvxpyLayer å®ç° (Benchmark)
# ==========================
class DifferentiableMPO_cvx(nn.Module):
    def __init__(self):
        super(DifferentiableMPO_cvx, self).__init__()
        try:
            import cvxpy as cp
            from cvxpylayers.torch import CvxpyLayer
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£… cvxpy å’Œ cvxpylayers: pip install cvxpy cvxpylayers")

        self.H = cfg.PREDICT_HORIZON
        self.N = cfg.NUM_ASSETS
        self.cfg_dict = {
            'gamma': cfg.RISK_AVERSION,
            'cost_coeff': cfg.COST_COEFF,
            'kappa': norm.pdf(norm.ppf(cfg.CVAR_CONFIDENCE)) / (1 - cfg.CVAR_CONFIDENCE),
            'cvar_penalty': getattr(cfg, 'CVAR_PENALTY', 50.0)
        }
        
        # 1. å®šä¹‰å‚æ•° (Parameters)
        self.mu_param = cp.Parameter((self.H, self.N))
        self.L_param = cp.Parameter((self.H, self.N, self.N))
        self.w_prev_param = cp.Parameter(self.N)
        self.cvar_limit_param = cp.Parameter() 
        
        # 2. å®šä¹‰å˜é‡ (Variables)
        self.w_var = cp.Variable((self.H, self.N))
        
        # 3. æ„å»ºç›®æ ‡å‡½æ•° (Loss_QP: Min Variance + L2 Cost)
        # ä¸å†åŒ…å«æ”¶ç›Šç‡é¡¹ (-mu^T w)
        # ç¥ç»ç½‘ç»œåªéœ€è¦é¢„æµ‹ Sigma (L)ï¼Œé€šè¿‡è°ƒæ•´é£é™©ç»“æ„æ¥é—´æ¥ä¼˜åŒ– Sortino
        obj = 0
        
        # (1) Risk: sum(||L_t^T w_t||^2)
        # è¿™ç°åœ¨æ˜¯ä¸»è¦çš„é©±åŠ¨é¡¹
        risk_term = 0
        for t in range(self.H):
            # L_t: (N, N), w_t: (N,)
            risk_term += cp.sum_squares(self.L_param[t].T @ self.w_var[t])
        obj += risk_term
        
        # (2) Cost: L2 Penalty (Smooth)
        # ä½¿ç”¨ L2 Norm æ›¿ä»£ L1ï¼Œä¿è¯ QP æ€§è´¨
        cost_term = 0
        # t=0
        cost_term += cp.sum_squares(self.w_var[0] - self.w_prev_param)
        # t=1..H-1
        for t in range(1, self.H):
            cost_term += cp.sum_squares(self.w_var[t] - self.w_var[t-1])
            
        # Cost Coeff éœ€è¦æ ¹æ® L2 çš„é‡çº§é‡æ–°è°ƒæ•´ï¼Œè¿™é‡Œæš‚æ—¶ä¿æŒ Config è¯»å–
        # ä½†é€šå¸¸ L2 cost éœ€è¦æ›´å¤§çš„ç³»æ•°æ‰èƒ½ä¸ Risk å¹³è¡¡
        obj += self.cfg_dict['cost_coeff'] * 10.0 * cost_term
        
        # (3) CVaR Penalty (REMOVED)
        # ç§»é™¤äº† CVaR é¡¹ï¼Œä¿æŒ Solver ä¸ºçº¯ QP
            
        # 4. çº¦æŸæ¡ä»¶
        constraints = [
            cp.sum(self.w_var, axis=1) == 1,
            self.w_var >= 0
        ]
        
        # 5. åˆå§‹åŒ– Layer
        # æ³¨æ„ï¼šä¸å†ä¼ å…¥ mu_param å’Œ cvar_limit_param
        problem = cp.Problem(cp.Minimize(obj), constraints)
        self.layer = CvxpyLayer(
            problem, 
            parameters=[self.L_param, self.w_prev_param], 
            variables=[self.w_var]
        )
        
    def forward(self, mu, L, w_prev, cvar_limit=None):
        # å…¼å®¹æ¥å£ï¼šè™½ç„¶ä¸å†ä½¿ç”¨ mu å’Œ cvar_limitï¼Œä½†ä¿æŒå‡½æ•°ç­¾åä¸€è‡´
        # mu: (Batch, H, N) -> IGNORED
        
        # è°ƒç”¨ CvxpyLayer
        w_star, = self.layer(L, w_prev)
        return w_star

# ==========================
# å•å…ƒæµ‹è¯• (Unit Test)
# ==========================
if __name__ == "__main__":
    import time
    import numpy as np
    
    # è®¾ç½®æ‰“å°ç²¾åº¦
    torch.set_printoptions(precision=4, sci_mode=False)
    
    print("ğŸ§ª å¼€å§‹å¯¹æ¯”æµ‹è¯•: Mirror Descent (MD) vs CvxpyLayer (CVX)...")
    
    # 1. æ¨¡æ‹Ÿ Batch æ•°æ®
    # ä½¿ç”¨è¾ƒå°çš„ Batch ä»¥ä¾¿ CVX è·‘å¾—åŠ¨ (CVX Batch æ€§èƒ½è¾ƒå·®)
    B, H, N = 4, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS
    device = cfg.DEVICE
    print(f"   Batch={B}, Horizon={H}, Assets={N}, Device={device}")
    
    # æ¨¡æ‹Ÿè¾“å…¥ (éœ€è¦æ¢¯åº¦)
    mu = torch.randn(B, H, N, requires_grad=True, dtype=torch.float32, device=device)
    L = torch.eye(N, device=device).view(1, 1, N, N).repeat(B, H, 1, 1)
    # å¢åŠ ä¸€ç‚¹éšæœºæ€§ç»™ L
    L = L + 0.1 * torch.randn_like(L)
    L.requires_grad = True
    
    w0 = torch.ones(B, N, dtype=torch.float32, device=device) / N
    w0.requires_grad = True # ä¹Ÿå¯ä»¥æµ‹è¯•å¯¹ w_prev çš„æ¢¯åº¦
    
    # 2. å®ä¾‹åŒ– Solvers
    print("\nğŸ“¦ åˆå§‹åŒ– Solvers...")
    solver_md = DifferentiableMPO().to(device)
    
    try:
        solver_cvx = DifferentiableMPO_cvx().to(device)
        has_cvx = True
        print("   âœ… DifferentiableMPO_cvx åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âš ï¸ DifferentiableMPO_cvx åŠ è½½å¤±è´¥: {e}")
        has_cvx = False
        
    if has_cvx:
        # ==========================
        # 3. å‰å‘ä¼ æ’­é€Ÿåº¦å¯¹æ¯”
        # ==========================
        print("\nğŸï¸  Forward Pass Speed Test (Avg of 10 runs)")
        
        # MD Warmup
        _ = solver_md(mu, L, w0)
        
        # MD Timing
        torch.cuda.synchronize() if device=='cuda' else None
        t0 = time.time()
        for _ in range(10):
            w_md = solver_md(mu, L, w0)
        torch.cuda.synchronize() if device=='cuda' else None
        t_md = (time.time() - t0) / 10
        print(f"   ğŸ”¹ Mirror Descent (Ours): {t_md*1000:.2f} ms")
        
        # CVX Warmup
        # CVX ç¬¬ä¸€æ¬¡è¿è¡Œé€šå¸¸å¾ˆæ…¢ (Canonicalization)ï¼ŒWarmup å¾ˆé‡è¦
        _ = solver_cvx(mu, L, w0)
        
        # CVX Timing
        torch.cuda.synchronize() if device=='cuda' else None
        t0 = time.time()
        for _ in range(10):
            w_cvx = solver_cvx(mu, L, w0)
        torch.cuda.synchronize() if device=='cuda' else None
        t_cvx = (time.time() - t0) / 10
        print(f"   ğŸ”¸ CvxpyLayer (Ref)   : {t_cvx*1000:.2f} ms")
        print(f"   ğŸš€ Speedup: {t_cvx / t_md:.1f}x")
        
        # ==========================
        # 4. ç»“æœä¸€è‡´æ€§å¯¹æ¯”
        # ==========================
        print("\nğŸ” Result Consistency Check")
        # æ¯”è¾ƒ w_md å’Œ w_cvx
        diff = torch.norm(w_md - w_cvx) / (torch.norm(w_cvx) + 1e-8)
        print(f"   Rel. Norm Diff: {diff.item():.6f}")
        if diff < 1e-2:
            print("   âœ… Results match closely.")
        else:
            print("   âš ï¸ Results might differ (check constraints/parameters).")
            
        # ==========================
        # 5. åå‘ä¼ æ’­é€Ÿåº¦ä¸æ¢¯åº¦å¯¹æ¯”
        # ==========================
        print("\nğŸ“‰ Backward Pass & Gradient Check")
        
        # æ„é€  Loss
        target = torch.rand_like(w_md)
        target = target / target.sum(dim=-1, keepdim=True)
        
        # --- MD Backward ---
        loss_md = torch.sum((w_md - target)**2)
        
        # æ¸…é›¶æ¢¯åº¦
        if mu.grad is not None: mu.grad.zero_()
        if L.grad is not None: L.grad.zero_()
        
        torch.cuda.synchronize() if device=='cuda' else None
        t0 = time.time()
        loss_md.backward(retain_graph=True)
        torch.cuda.synchronize() if device=='cuda' else None
        t_md_back = time.time() - t0
        print(f"   ğŸ”¹ MD Backward Time : {t_md_back*1000:.2f} ms")
        
        grad_mu_md = mu.grad.clone()
        mu.grad.zero_() # Reset for CVX
        
        # --- CVX Backward ---
        # å¿…é¡»é‡æ–°è®¡ç®— Graphï¼Œå› ä¸º w_cvx å’Œ w_md æ˜¯ä¸åŒçš„è®¡ç®—å›¾èŠ‚ç‚¹
        # ä¸ºäº†å…¬å¹³ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥å¯¹ w_cvx backward
        loss_cvx = torch.sum((w_cvx - target)**2)
        
        torch.cuda.synchronize() if device=='cuda' else None
        t0 = time.time()
        loss_cvx.backward()
        torch.cuda.synchronize() if device=='cuda' else None
        t_cvx_back = time.time() - t0
        print(f"   ğŸ”¸ CVX Backward Time: {t_cvx_back*1000:.2f} ms")
        
        grad_mu_cvx = mu.grad.clone()
        
        # --- Gradient Comparison ---
        grad_diff = torch.norm(grad_mu_md - grad_mu_cvx) / (torch.norm(grad_mu_cvx) + 1e-6)
        print(f"   Gradient Rel. Diff (Mu): {grad_diff.item():.6f}")
        
        # Cosine Similarity
        cos_sim = torch.nn.functional.cosine_similarity(grad_mu_md.flatten(), grad_mu_cvx.flatten(), dim=0)
        print(f"   Gradient Cosine Sim    : {cos_sim.item():.4f}")
        
    print("\nDone.")

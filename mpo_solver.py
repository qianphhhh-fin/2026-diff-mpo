import torch
import torch.nn as nn
import cvxpy as cp
from cvxpylayers.torch import CvxpyLayer
from config import cfg

class DifferentiableMPO(nn.Module):
    def __init__(self):
        super(DifferentiableMPO, self).__init__()
        
        # ==========================================
        # 1. å®šä¹‰ç¬¦å·å˜é‡
        # ==========================================
        H = cfg.PREDICT_HORIZON
        N = cfg.NUM_ASSETS
        
        # âš ï¸ ä¿®æ­£ç‚¹ 1: åªæœ‰ mu, L, w0 æ˜¯éœ€è¦åå‘ä¼ æ’­çš„ Parameter
        self.param_mu = cp.Parameter((H, N), name='mu') 
        self.param_L  = cp.Parameter((H, N, N), name='L') 
        self.param_w0 = cp.Parameter(N, name='w_prev') 
        
        # âš ï¸ ä¿®æ­£ç‚¹ 2: gamma å’Œ cost_coeff æ”¹ä¸ºæ™®é€š Python å˜é‡ (å¸¸é‡)
        # ä¸è¦ç”¨ cp.Parameter åŒ…è£…ï¼Œå¦åˆ™ä¼šç ´å DPP ç»“æ„
        gamma = cfg.RISK_AVERSION
        cost_coeff = cfg.COST_COEFF
        
        # å†³ç­–å˜é‡
        w = cp.Variable((H, N), name='w_plan')
        
        # ==========================================
        # 2. æ„å»ºç›®æ ‡å‡½æ•°ä¸çº¦æŸ
        # ==========================================
        obj_ret = 0
        obj_risk = 0
        obj_cost = 0
        constraints = []
        
        w_current = self.param_w0
        
        for t in range(H):
            # A. æ”¶ç›Š
            obj_ret += self.param_mu[t] @ w[t]
            
            # B. é£é™©
            obj_risk += cp.sum_squares(self.param_L[t].T @ w[t])
            
            # C. äº¤æ˜“æˆæœ¬
            obj_cost += cp.norm(w[t] - w_current, 1)
            
            # D. çº¦æŸ
            constraints.append(cp.sum(w[t]) == 1.0)
            constraints.append(w[t] >= 0)
            
            w_current = w[t]
            
        # âš ï¸ ä¿®æ­£ç‚¹ 3: è¿™é‡Œæ˜¯ Float * Convexï¼Œç¬¦åˆ DPP
        objective = cp.Maximize(obj_ret - gamma * obj_risk - cost_coeff * obj_cost)
        
        # ==========================================
        # 3. åˆ›å»º CvxpyLayer
        # ==========================================
        problem = cp.Problem(objective, constraints)
        
        # ç°åœ¨è¿™ä¸€è¡Œåº”è¯¥èƒ½é€šè¿‡äº†
        assert problem.is_dpp(), "é—®é¢˜ä¸ç¬¦åˆ DPP è§„åˆ™ï¼è¯·æ£€æŸ¥æ˜¯å¦ç”¨ Parameter ä¹˜ä»¥äº†å‡¸é¡¹ã€‚"
        
        self.layer = CvxpyLayer(
            problem, 
            parameters=[self.param_mu, self.param_L, self.param_w0], 
            variables=[w]
        )
        
    def forward(self, mu, L, w_prev):
        # solver_args åŠ ä¸Š eps å¯ä»¥é˜²æ­¢æ•°å€¼é—®é¢˜æŠ¥é”™
        w_plan, = self.layer(
            mu, L, w_prev, 
            solver_args={
                'solve_method': 'ECOS',
                'abstol': 1e-4, # æ”¾å®½ä¸€ç‚¹ç²¾åº¦ï¼Œè®­ç»ƒæ›´å¿«
                'reltol': 1e-4
            }
        )
        return w_plan

# ==========================
# å•å…ƒæµ‹è¯• (Unit Test)
# ==========================
if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹æµ‹è¯• mpo_solver æ¨¡å— (Gradient Check)...")
    
    # 1. æ¨¡æ‹Ÿ Batch æ•°æ®
    B, H, N = 2, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS
    
    # æ¨¡æ‹Ÿé¢„æµ‹çš„ Mu (éœ€è¦æ¢¯åº¦)
    mu = torch.randn(B, H, N, requires_grad=True, dtype=torch.double) # CVXPY é»˜è®¤å–œæ¬¢ double
    
    # æ¨¡æ‹Ÿé¢„æµ‹çš„ L (éœ€è¦æ¢¯åº¦) - åˆå§‹åŒ–ä¸ºå•ä½é˜µé™„è¿‘
    # L å¿…é¡»æ˜¯ä¸‹ä¸‰è§’ï¼Œè¿™é‡Œç®€åŒ–ï¼Œå‡è®¾ç½‘ç»œè¾“å‡ºå…¨çŸ©é˜µï¼Œä½†é€»è¾‘ä¸Šå®ƒæ˜¯ Factor
    L = torch.eye(N).view(1, 1, N, N).repeat(B, H, 1, 1).double()
    L.requires_grad = True
    
    # åˆå§‹æƒé‡ (ä¸éœ€è¦æ¢¯åº¦)
    w0 = torch.ones(B, N, dtype=torch.double) / N
    
    # 2. å®ä¾‹åŒ– Solver
    # try:
    solver = DifferentiableMPO()
    print("âœ… Solver åˆå§‹åŒ–æˆåŠŸ (Problem Compiled)")
    
    # 3. å‰å‘ä¼ æ’­
    w_plan = solver(mu, L, w0)
    print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ. Output Shape: {w_plan.shape} (Expected: {B, H, N})")
    
    # 4. åå‘ä¼ æ’­æµ‹è¯•
    # æ„é€ ä¸€ä¸ªå‡çš„ Loss: å¸Œæœ› w çš„ç¬¬ä¸€ä¸ªèµ„äº§æƒé‡è¶Šå¤§è¶Šå¥½
    loss = -w_plan[:, :, 0].sum()
    loss.backward()
    
    print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
    print(f"   Gradient of mu exists: {mu.grad is not None}")
    print(f"   Gradient of L exists: {L.grad is not None}")
    print(f"   mu grad sample: {mu.grad[0,0,:]}")
    
    print("\nğŸš€ mpo_solver æ¨¡å—é€šè¿‡ï¼æ ¸å¿ƒå¼•æ“å°±ç»ªã€‚")
        
    # except Exception as e:
    #     print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
    #     print("æç¤ºï¼šå¦‚æœæ˜¯ SolverErrorï¼Œå¯èƒ½æ˜¯æ•°æ®éšæœºåˆå§‹åŒ–å¯¼è‡´æ— è§£ï¼Œæˆ–è€…ç¼ºå°‘ ECOS/SCS æ±‚è§£å™¨ã€‚")
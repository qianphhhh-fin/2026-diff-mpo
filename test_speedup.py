
"""
è„šæœ¬åç§°: test_speedup.py
åŠŸèƒ½æè¿°: 
    ç‹¬ç«‹æµ‹è¯•è„šæœ¬ï¼Œç”¨äºŽæ¯”è¾ƒ Diff-MPO åœ¨æŽ¨ç†é˜¶æ®µ (Inference) çš„ä¸åŒå®žçŽ°æ–¹å¼çš„é€Ÿåº¦å’Œç»“æžœä¸€è‡´æ€§ã€‚
    é‡ç‚¹æ¯”è¾ƒä»¥ä¸‹ä¸¤ç§æ¨¡å¼ï¼š
    1. Baseline (Old): æ¨¡æ‹Ÿ eval_rolling_all.py ä¸­çš„é€æ—¥æŽ¨ç†ï¼Œæ¯æ¬¡ Batch=1ï¼Œé¢‘ç¹ IOã€‚
    2. Optimized (New): æ‰¹é‡æŽ¨ç†æ¨¡å¼ï¼Œä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ—¶é—´æ­¥çš„æ•°æ®ï¼Œåˆ©ç”¨çŸ©é˜µå¹¶è¡ŒåŠ é€Ÿã€‚

    æ³¨æ„ï¼šOptimized æ¨¡å¼åœ¨å›žæµ‹ä¸­æ„å‘³ç€æˆ‘ä»¬ä¸€æ¬¡æ€§é¢„æµ‹æœªæ¥ N å¤©ï¼Œä½†åœ¨çœŸå®žå›žæµ‹ä¸­ï¼Œ
    æˆ‘ä»¬åªèƒ½åˆ©ç”¨ t æ—¶åˆ»çš„ä¿¡æ¯é¢„æµ‹ t+1ï¼Œä¸èƒ½å·çœ‹æœªæ¥ã€‚
    å› æ­¤ï¼Œ"åˆæ³•"çš„åŠ é€Ÿæ–¹å¼æ˜¯ï¼š
    - é¢„å…ˆå‡†å¤‡å¥½æ‰€æœ‰ t æ—¶åˆ»çš„ feature tensor (Batch=Total_Days)ã€‚
    - ä¸€æ¬¡æ€§å–‚ç»™æ¨¡åž‹ (Batch Inference)ã€‚
    - å¾—åˆ°æ‰€æœ‰å¤©æ•°çš„é¢„æµ‹ç»“æžœ (mu, L)ã€‚
    - ç„¶åŽåªåœ¨ Solver å±‚é¢è¿›è¡Œé€æ—¥è¿­ä»£ (å› ä¸º w_prev ä¾èµ–äºŽå‰ä¸€å¤©çš„ w)ã€‚
    - ç”šè‡³ Solver ä¹Ÿå¯ä»¥å¹¶è¡ŒåŒ–ï¼Ÿä¸ï¼ŒSolver æ˜¯ä¸²è¡Œçš„ (Stateful)ï¼Œé™¤éžæˆ‘ä»¬å¿½ç•¥ Transaction Cost çš„åŠ¨æ€å½±å“ã€‚
    - ä½†å¤§éƒ¨åˆ†è€—æ—¶å¯èƒ½åœ¨ LSTM å’Œ Heads ä¸Šï¼Œæ‰€ä»¥ Batch Inference èƒ½åŠ é€Ÿ Model éƒ¨åˆ†ã€‚

    æœ¬è„šæœ¬æµ‹è¯•ï¼š
    "é€æ—¥ Model + é€æ—¥ Solver" vs "æ‰¹é‡ Model + é€æ—¥ Solver"
"""

import torch
import pandas as pd
import numpy as np
import time
from torch.utils.data import DataLoader
from tqdm import tqdm

from config import cfg
from data_loader import MPODataset
from model import MPO_Network_Factor
from mpo_solver import DifferentiableMPO_cvx

def run_speedup_benchmark():
    print("ðŸŽï¸ å¼€å§‹ Diff-MPO æŽ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯• (Speedup Benchmark)...")
    print(f"   Device: {cfg.DEVICE}")
    
    # ==========================
    # 1. å‡†å¤‡æ•°æ® (Data Chunk)
    # ==========================
    # åŠ è½½ 1000 å¤©çš„æ•°æ®ç”¨äºŽæµ‹è¯•
    # ç¡®ä¿æœ‰è¶³å¤Ÿ Lookback
    N_DAYS = 1000
    print(f"   -> Loading {N_DAYS} days of data...")
    
    df_raw = pd.read_csv(cfg.DATA_PATH, index_col=0, parse_dates=True)
    all_features = df_raw.values
    all_returns = df_raw[cfg.ASSETS].values
    
    # æž„é€  N_DAYS ä¸ªæ ·æœ¬
    # æ¯ä¸ªæ ·æœ¬ Input: (Lookback, Feat)
    X_list = []
    Y_list = []
    
    start_idx = cfg.LOOKBACK_WINDOW
    end_idx = start_idx + N_DAYS
    
    # ä¸ºäº†æ¨¡æ‹Ÿ eval_rolling_allï¼Œæˆ‘ä»¬éœ€è¦é€ä¸ªåˆ‡ç‰‡
    for t in range(start_idx, end_idx):
        x_window = all_features[t - cfg.LOOKBACK_WINDOW : t]
        # ç®€å•å½’ä¸€åŒ– (æ¨¡æ‹Ÿ scaler)
        x_window = (x_window - x_window.mean(axis=0)) / (x_window.std(axis=0) + 1e-6)
        X_list.append(x_window)
        Y_list.append(all_returns[t]) # Dummy Y
        
    X_tensor = torch.tensor(np.array(X_list), dtype=torch.double) # (N, Lookback, Feat)
    
    print(f"   -> Input Tensor Shape: {X_tensor.shape}")
    
    # ==========================
    # 2. åˆå§‹åŒ–æ¨¡åž‹
    # ==========================
    model = MPO_Network_Factor().to(cfg.DEVICE).double()
    model.eval()
    
    # åˆå§‹æŒä»“
    w_init = torch.ones(1, cfg.NUM_ASSETS, device=cfg.DEVICE, dtype=torch.double) / cfg.NUM_ASSETS
    
    # ==========================
    # 3. Baseline: é€æ—¥å¾ªçŽ¯ (Day-by-Day Loop)
    # ==========================
    print("\nðŸ¢ Running Baseline (Day-by-Day)...")
    
    # å¿…é¡»æŠŠ Tensor æ‹†å›ž CPU åˆ—è¡¨æ¥æ¨¡æ‹ŸçœŸå®žåœºæ™¯çš„ IO
    # åœ¨ eval_rolling_all ä¸­ï¼Œæ¯æ¬¡æ˜¯ä»Ž numpy -> tensor -> gpu
    X_cpu_list = [t.unsqueeze(0) for t in X_tensor] 
    
    w_prev = w_init.clone()
    results_baseline = []
    
    start_time = time.time()
    
    for i in tqdm(range(N_DAYS), desc="Baseline"):
        # 1. IO Overhead
        x_day = X_cpu_list[i].to(cfg.DEVICE)
        
        # 2. Model Inference
        with torch.no_grad():
            w_plan, _, _ = model(x_day, w_prev)
        
        # 3. State Update
        w_action = w_plan[:, 0, :] # (1, N)
        w_prev = w_action
        
        # 4. Result Retrieval
        results_baseline.append(w_action.cpu().numpy())
        
    time_baseline = time.time() - start_time
    print(f"   -> Baseline Time: {time_baseline:.4f}s ({N_DAYS/time_baseline:.1f} iter/s)")
    
    # ==========================
    # 4. Optimized: æ‰¹é‡æ¨¡åž‹ + ä¸²è¡Œæ±‚è§£ (Batch Model + Serial Solver)
    # ==========================
    print("\nðŸ‡ Running Optimized (Batch Model + Serial Solver)...")
    
    start_time_opt = time.time()
    
    # 1. Batch Model Inference
    # ä¸€æ¬¡æ€§å°†æ‰€æœ‰ X æŽ¨å…¥ GPU è®¡ç®— mu å’Œ L
    # Batch Size å¯ä»¥å¾ˆå¤§ï¼Œæ¯”å¦‚ 1000
    BATCH_SIZE_LARGE = 256
    dataset = torch.utils.data.TensorDataset(X_tensor)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE_LARGE, shuffle=False)
    
    mu_list = []
    L_list = []
    
    with torch.no_grad():
        for (x_batch,) in loader:
            x_batch = x_batch.to(cfg.DEVICE)
            
            # æˆ‘ä»¬åªç”¨ model çš„ Encoder å’Œ Head éƒ¨åˆ†
            # è¿™ä¸€æ­¥éœ€è¦æ‹†è§£ model.forwardï¼Œæˆ–è€…ç»™ model åŠ ä¸€ä¸ªåªè¾“å‡ºå‚æ•°çš„æŽ¥å£
            # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨ model çš„å­æ¨¡å— (White-box Optimization)
            
            # --- Model Internal Logic ---
            _, (h_n, _) = model.lstm(x_batch)
            context = h_n[-1]
            
            mu = model.mu_head(context).view(-1, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS)
            
            B_flat = model.B_head(context)
            B = B_flat.view(-1, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS, cfg.NUM_FACTORS)
            D_flat = model.D_head(context)
            D = torch.nn.functional.softplus(D_flat) + 1e-3
            D = D.view(-1, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS)
            
            factor_cov = torch.matmul(B, B.transpose(-1, -2))
            idiosyncratic_cov = torch.diag_embed(D**2)
            Sigma = factor_cov + idiosyncratic_cov
            
            epsilon_eye = 1e-5 * torch.eye(cfg.NUM_ASSETS, device=cfg.DEVICE).view(1, 1, cfg.NUM_ASSETS, cfg.NUM_ASSETS)
            Sigma_stabilized = Sigma + epsilon_eye
            try:
                L = torch.linalg.cholesky(Sigma_stabilized)
            except RuntimeError:
                L = torch.diag_embed(D + 1e-3)
            # ----------------------------
            
            mu_list.append(mu)
            L_list.append(L)
            
    # æ‹¼æŽ¥æ‰€æœ‰é¢„æµ‹å‚æ•°
    mu_all = torch.cat(mu_list, dim=0)
    L_all = torch.cat(L_list, dim=0)
    
    # 2. Serial Solver Loop
    # è¿™ä¸€æ­¥æ— æ³•å¹¶è¡Œï¼Œå› ä¸º w_t ä¾èµ– w_{t-1}
    # ä½†æˆ‘ä»¬çœåŽ»äº† LSTM çš„é‡å¤è®¡ç®—å’Œ IO
    
    w_prev = w_init.clone()
    results_opt = []
    
    # æå– Solver
    solver = model.mpo_layer
    cvar_limit = torch.tensor(cfg.CVAR_LIMIT, device=cfg.DEVICE, dtype=torch.double).expand(1)
    
    for i in tqdm(range(N_DAYS), desc="Optimized"):
        # å–å‡ºç¬¬ i å¤©çš„å‚æ•° (1, H, N)
        mu_day = mu_all[i:i+1]
        L_day = L_all[i:i+1]
        
        # çº¯ Solver è®¡ç®—
        # DifferentiableMPO_cvx è°ƒç”¨ CvxpyLayer
        with torch.no_grad():
            w_plan = solver(mu_day, L_day, w_prev, cvar_limit)
        
        w_action = w_plan[:, 0, :]
        w_prev = w_action
        results_opt.append(w_action.cpu().numpy())
        
    time_opt = time.time() - start_time_opt
    print(f"   -> Optimized Time: {time_opt:.4f}s ({N_DAYS/time_opt:.1f} iter/s)")
    
    # ==========================
    # 5. ç»“æžœå¯¹æ¯”
    # ==========================
    print("\nâš–ï¸ ç»“æžœä¸€è‡´æ€§æ£€æŸ¥...")
    
    res_base = np.concatenate(results_baseline, axis=0)
    res_opt = np.concatenate(results_opt, axis=0)
    
    diff = np.abs(res_base - res_opt).max()
    print(f"   -> Max Difference: {diff:.6e}")
    
    if diff < 1e-6:
        print("   âœ… ç»“æžœä¸€è‡´ï¼ä¼˜åŒ–æ–¹æ¡ˆæœ‰æ•ˆã€‚")
        speedup = time_baseline / time_opt
        print(f"   ðŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
    else:
        print("   âŒ ç»“æžœä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥é€»è¾‘ã€‚")

if __name__ == "__main__":
    run_speedup_benchmark()

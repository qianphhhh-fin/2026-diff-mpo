import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from tqdm import tqdm
import os
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader

# å¼•å…¥åŸºç¡€é…ç½®
from config import cfg
from data_loader import MPODataset
# æ³¨æ„ï¼šmpo_solver å¿…é¡»å¼•ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ DifferentiableMPO å±‚
from mpo_solver import DifferentiableMPO
# å¼•å…¥ Loss è®¡ç®—
from train_diff_mpo import calc_composite_loss 

# è®¾ç½®é£æ ¼
plt.style.use('seaborn-v0_8')
device = cfg.DEVICE

# ==========================================
# 0. ä¸´æ—¶å®šä¹‰æ–°æ¨¡å‹ï¼šç»“æ„åŒ–åæ–¹å·® (Factor Model)
# ==========================================
class MPO_Network_Factor(nn.Module):
    def __init__(self, input_dim, num_assets, hidden_dim=64):
        super(MPO_Network_Factor, self).__init__()
        
        # 1. ç‰¹å¾æå–å™¨
        self.lstm = nn.LSTM(
            input_size=input_dim, 
            hidden_size=hidden_dim,
            num_layers=cfg.NUM_LAYERS,
            batch_first=True,
            dropout=cfg.DROPOUT
        )
        
        # 2. é¢„æµ‹å¤´
        
        # Head A: æ”¶ç›Šç‡ mu (ä¸å˜)
        self.mu_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Dropout(cfg.DROPOUT),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets)
        )
        
        # Head B: ç»“æ„åŒ–åæ–¹å·® (Structured Covariance)
        # å‡è®¾å­˜åœ¨ K ä¸ªéšå› å­ (Latent Factors)
        # ç»éªŒæ³•åˆ™: K < N. è¿™é‡Œæˆ‘ä»¬è®¾å®š K=3 (å¯¹åº” Market, Size, Value ç­‰å®è§‚åŠ›é‡)
        self.num_factors = 3 
        self.num_assets = num_assets
        
        # é¢„æµ‹å› å­è½½è· B (Batch, H, N, K)
        # è¿™ä»£è¡¨æ¯ä¸ªèµ„äº§å¯¹ 3 ä¸ªéšå› å­çš„æ•æ„Ÿåº¦
        self.B_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets * self.num_factors)
        )
        
        # é¢„æµ‹ç‰¹å¼‚æ€§æ³¢åŠ¨ D (Batch, H, N)
        # è¿™ä»£è¡¨æ¯ä¸ªèµ„äº§ç‰¹æœ‰çš„ã€ä¸èƒ½è¢«å› å­è§£é‡Šçš„æ³¢åŠ¨
        self.D_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets)
        )
        
        # 3. ä¼˜åŒ–å±‚
        self.mpo_layer = DifferentiableMPO()
        
    def forward(self, x, w_prev):
        batch_size = x.size(0)
        
        # --- Encoding ---
        _, (h_n, _) = self.lstm(x)
        context = h_n[-1]
        
        # --- Parameter Prediction ---
        
        # 1. Mu
        mu = self.mu_head(context)
        mu = mu.view(batch_size, cfg.PREDICT_HORIZON, self.num_assets)
        
        # 2. Sigma (Factor Model Construction)
        # B: (Batch, H, N, K)
        B_flat = self.B_head(context)
        B = B_flat.view(batch_size, cfg.PREDICT_HORIZON, self.num_assets, self.num_factors)
        
        # D: (Batch, H, N) -> å¿…é¡»ä¸ºæ­£
        D_flat = self.D_head(context)
        D = F.softplus(D_flat) + 1e-4 # ä¿è¯å¤§äº0
        D = D.view(batch_size, cfg.PREDICT_HORIZON, self.num_assets)
        
        # æ„é€ åæ–¹å·®çŸ©é˜µ Sigma = B @ B.T + diag(D^2)
        # è¿™ç§æ„é€ æ–¹å¼å¤©ç„¶ä¿è¯ Sigma æ˜¯å¯¹ç§°æ­£å®šçš„ (SPSD)
        
        # B @ B.T -> (Batch, H, N, N)
        factor_cov = torch.matmul(B, B.transpose(-1, -2)) 
        
        # Idiosyncratic variance matrix
        # torch.diag_embed ä¼šæŠŠ D^2 æ”¾åˆ°å¯¹è§’çº¿ä¸Š
        idiosyncratic_cov = torch.diag_embed(D**2)
        
        Sigma = factor_cov + idiosyncratic_cov
        
        # --- Cholesky Decomposition ---
        # Solver éœ€è¦ L (where Sigma = L @ L.T)
        # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼ŒåŠ ä¸€ä¸ªå°æ‰°åŠ¨
        Sigma_stabilized = Sigma + 1e-6 * torch.eye(self.num_assets, device=x.device).view(1, 1, self.num_assets, self.num_assets)
        
        try:
            L = torch.linalg.cholesky(Sigma_stabilized)
        except RuntimeError:
            # å¦‚æœä¸‡ä¸€ç‚¸äº†ï¼ˆæå°‘æƒ…å†µï¼‰ï¼Œå›é€€åˆ°åªç”¨ç‰¹å¼‚æ€§æ³¢åŠ¨ (å¯¹è§’é˜µ)
            L = torch.diag_embed(D + 1e-3)
        
        # --- Optimization ---
        # è½¬ç§»åˆ° CPU ç»™ cvxpylayers è®¡ç®—
        mu_cpu = mu.cpu()
        L_cpu = L.cpu()
        w_prev_cpu = w_prev.cpu()
        
        w_plan_cpu = self.mpo_layer(mu_cpu, L_cpu, w_prev_cpu)
        w_plan = w_plan_cpu.to(x.device)
        
        return w_plan, mu, L


class MPO_Transformer_Factor(nn.Module):
    def __init__(self, input_dim, num_assets, lookback_window, hidden_dim=64, nhead=4, num_layers=2):
        super(MPO_Transformer_Factor, self).__init__()
        
        # 1. Input Projection & Positional Encoding
        # å°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ° d_model ç»´åº¦
        self.embedding = nn.Linear(input_dim, hidden_dim)
        
        # å¯å­¦ä¹ çš„ä½ç½®ç¼–ç  (Learnable Positional Encoding)
        # Shape: (1, Lookback, Hidden) -> å¹¿æ’­åˆ° Batch
        self.pos_encoder = nn.Parameter(torch.randn(1, lookback_window, hidden_dim) * 0.02)
        
        # 2. Transformer Backbone
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, 
            nhead=nhead, 
            dim_feedforward=hidden_dim * 4,
            dropout=cfg.DROPOUT,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # 3. é¢„æµ‹å¤´ (ä¸ Factor Model ä¿æŒä¸€è‡´)
        self.num_assets = num_assets
        self.num_factors = 3 
        
        # Head A: æ”¶ç›Šç‡ mu
        self.mu_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Dropout(cfg.DROPOUT),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets)
        )
        
        # Head B: ç»“æ„åŒ–åæ–¹å·® (B & D)
        # å› å­è½½è· B
        self.B_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets * self.num_factors)
        )
        # ç‰¹å¼‚æ€§æ³¢åŠ¨ D
        self.D_head = nn.Sequential(
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            nn.Linear(32, cfg.PREDICT_HORIZON * num_assets)
        )
        
        # 4. ä¼˜åŒ–å±‚
        self.mpo_layer = DifferentiableMPO()
        
    def forward(self, x, w_prev):
        # x: (Batch, Lookback, Features)
        batch_size = x.size(0)
        seq_len = x.size(1)
        
        # --- Transformer Encoding ---
        # 1. Embedding + Positional Encoding
        # æ³¨æ„ï¼šå¦‚æœå®é™…è¾“å…¥é•¿åº¦å°äº lookback (æå°‘æƒ…å†µ)ï¼Œåˆ‡ç‰‡ pos_encoder
        x_embed = self.embedding(x) + self.pos_encoder[:, :seq_len, :]
        
        # 2. Attention
        # Transformer è¾“å‡º: (Batch, Lookback, Hidden)
        x_trans = self.transformer_encoder(x_embed)
        
        # 3. Aggregation
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ä½œä¸º Context (ç±»ä¼¼ LSTM çš„ h_n[-1])
        context = x_trans[:, -1, :] 
        
        # --- Parameter Prediction (é€»è¾‘ä¸ LSTM ç‰ˆå®Œå…¨ä¸€è‡´) ---
        
        # 1. Mu
        mu = self.mu_head(context).view(batch_size, cfg.PREDICT_HORIZON, self.num_assets)
        
        # 2. Sigma (Factor Model)
        B_flat = self.B_head(context)
        B = B_flat.view(batch_size, cfg.PREDICT_HORIZON, self.num_assets, self.num_factors)
        
        D_flat = self.D_head(context)
        D = F.softplus(D_flat) + 1e-4
        D = D.view(batch_size, cfg.PREDICT_HORIZON, self.num_assets)
        
        # Sigma = B @ B.T + D^2
        factor_cov = torch.matmul(B, B.transpose(-1, -2)) 
        idiosyncratic_cov = torch.diag_embed(D**2)
        Sigma = factor_cov + idiosyncratic_cov
        
        # Cholesky
        Sigma_stabilized = Sigma + 1e-6 * torch.eye(self.num_assets, device=x.device).view(1, 1, self.num_assets, self.num_assets)
        try:
            L = torch.linalg.cholesky(Sigma_stabilized)
        except RuntimeError:
            L = torch.diag_embed(D + 1e-3)
        
        # --- Optimization ---
        w_plan = self.mpo_layer(mu.cpu(), L.cpu(), w_prev.cpu()).to(x.device)
        
        return w_plan, mu, L
    

# ==========================================
# 1. æ»šåŠ¨å›æµ‹ä¸»ç¨‹åº
# ==========================================
def run_walk_forward_experiment():
    print("âš”ï¸ [Walk-Forward Experiment] Diff-MPO (Factor Model) vs 1/N ...")
    
    # 1. å‡†å¤‡å…¨é‡æ•°æ®
    df_raw = pd.read_csv(cfg.DATA_PATH, index_col=0, parse_dates=True)
    
    # è‡ªåŠ¨è·å–ç‰¹å¾ç»´åº¦ (ä¸å†ä¾èµ– model.py é‡Œçš„ç¡¬ç¼–ç )
    all_features = df_raw.values 
    input_feature_dim = all_features.shape[1]
    
    # ç›®æ ‡èµ„äº§
    all_returns = df_raw[cfg.ASSETS].values
    dates = df_raw.index
    
    # è®¾ç½®å›æµ‹æ—¶é—´è½´ (å»ºè®®ä» 2018 å¼€å§‹)
    TEST_START_YEAR = 2018 
    TEST_END_YEAR = dates[-1].year
    
    # åˆå§‹åŒ–è®°å½•å™¨
    results_dmpo = [] 
    results_ew = []   
    
    # åˆå§‹åŒ–æ–°æ¨¡å‹ (Factor Model)
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æˆ‘ä»¬åœ¨è„šæœ¬é‡Œå®šä¹‰çš„ MPO_Network_Factor
    # model = MPO_Network_Factor(
    #     input_dim=input_feature_dim,
    #     num_assets=cfg.NUM_ASSETS,
    #     hidden_dim=cfg.HIDDEN_DIM
    # ).to(device).double()
    # print(f"   æ¨¡å‹æ¶æ„: Factor Model (3 Latent Factors)")



    # åˆå§‹åŒ–æ–°æ¨¡å‹ (Transformer + Factor Model)
    # æ³¨æ„ï¼šè¿™é‡Œæ”¹ç”¨ Transformer ç‰ˆæœ¬
    model = MPO_Transformer_Factor(
        input_dim=input_feature_dim,
        num_assets=cfg.NUM_ASSETS,
        lookback_window=cfg.LOOKBACK_WINDOW, # <--- å¿…é¡»ä¼ å…¥æ­¤å‚æ•°
        hidden_dim=cfg.HIDDEN_DIM,
        nhead=4,      # 4 å¤´æ³¨æ„åŠ› (64/4=16 dim per head)
        num_layers=2  # 2 å±‚ Transformer Block
    ).to(device).double()

    print(f"   æ¨¡å‹æ¶æ„: Transformer Factor Model")
    
    print(f"   è¾“å…¥ç»´åº¦: {input_feature_dim}, èµ„äº§æ•°: {cfg.NUM_ASSETS}")
    print(f"   å›æµ‹åŒºé—´: {TEST_START_YEAR} -> {TEST_END_YEAR}")    

    
    # 2. æ»šåŠ¨å¾ªç¯
    for year in range(TEST_START_YEAR, TEST_END_YEAR + 1):
        print(f"\nğŸ“… æ­£åœ¨å¤„ç†å¹´ä»½: {year} ...")
        
        # --- A. æ—¶é—´åˆ‡åˆ† (Expanding Window) ---
        train_end_dt = pd.Timestamp(f"{year}-01-01")
        test_end_dt = pd.Timestamp(f"{year+1}-01-01")
        
        train_mask = dates < train_end_dt
        test_mask = (dates >= train_end_dt) & (dates < test_end_dt)
        
        if sum(test_mask) < cfg.LOOKBACK_WINDOW:
            print(f"   âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ {year}")
            continue
            
        # --- B. æ•°æ®é˜²æ³„æ¼å¤„ç† ---
        scaler = StandardScaler()
        X_train = scaler.fit_transform(all_features[train_mask])
        Y_train = all_returns[train_mask]
        
        X_test = scaler.transform(all_features[test_mask])
        Y_test = all_returns[test_mask] 
        test_dates_curr = dates[test_mask]
        
        # æ„å»º DataLoader
        train_ds = MPODataset(X_train, Y_train, cfg.LOOKBACK_WINDOW, cfg.PREDICT_HORIZON)
        # Drop last ä¿è¯ Batch å®Œæ•´ï¼Œshuffle å¢åŠ æ³›åŒ–
        train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)
        
        # --- C. æ¨¡å‹å¾®è°ƒ (Fine-tune) ---
        # æ¯å¹´ 10 Epochsï¼Œå­¦ä¹ ç‡ 5e-4
        optimizer = optim.Adam(model.parameters(), lr=5e-4) 
        model.train()
        
        train_pbar = tqdm(range(10), desc=f"   Training {year}", leave=False)
        for ep in train_pbar:
            ep_loss = 0
            for x_b, y_b in train_loader:
                x_b, y_b = x_b.to(device).double(), y_b.to(device).double()
                
                # å‡è®¾ w_prev æ¯å¤©é‡ç½®ä¸º 1/N 
                w_prev_b = torch.ones(x_b.size(0), cfg.NUM_ASSETS, device=device, dtype=torch.double) / cfg.NUM_ASSETS
                
                w_plan, _, _ = model(x_b, w_prev_b)
                
                # Loss è®¡ç®— (åŒ…å« MaxDD å’Œ Turnover æƒ©ç½š)
                loss, _ = calc_composite_loss(w_plan, y_b, w_prev_b, cost_coeff=cfg.COST_COEFF)
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                ep_loss += loss.item()
            train_pbar.set_postfix({'loss': f"{ep_loss/len(train_loader):.4f}"})
            
# --- D. æ ·æœ¬å¤–é¢„æµ‹ (Rolling Inference) ---
        model.eval()
        
        # 1. å‡†å¤‡ Inference æ•°æ®
        # ä¿®æ­£ï¼šæˆ‘ä»¬éœ€è¦å›æº¯æ›´å¤šçš„æ•°æ®ï¼Œä»¥ä¾¿åš Lag
        # infer_start_idx ä¿æŒä¸å˜ï¼Œè¿˜æ˜¯å›æº¯ Lookback-1 (è¿™æ˜¯ä¸ºäº†å¯¹é½ Y[0] çš„æ—¶é—´ç‚¹)
        test_start_idx = np.where(test_mask)[0][0]
        # æˆ‘ä»¬å¤šå– 1 å¤©æ•°æ®ï¼Œé˜²æ­¢æ•°ç»„è¶Šç•Œï¼Œä½†æ ¸å¿ƒæ˜¯åœ¨ loop é‡Œæ§åˆ¶
        infer_start_idx = max(0, test_start_idx - cfg.LOOKBACK_WINDOW) 
        
        # æ³¨æ„ï¼šè¿™é‡Œå–å‡ºçš„ X_infer_raw åŒ…å«äº† "æ˜¨å¤©" å’Œ "ä»Šå¤©" çš„æ•°æ®
        X_infer_raw = all_features[infer_start_idx : test_start_idx + len(test_dates_curr)]
        X_infer_scaled = scaler.transform(X_infer_raw)
        
        Y_realized = Y_test 
        
        curr_w = torch.ones(1, cfg.NUM_ASSETS, device=device, dtype=torch.double) / cfg.NUM_ASSETS
        
        with torch.no_grad():
            for t in range(len(Y_realized)):
                # ================= æ ¸å¿ƒä¿®å¤ =================
                # ç›®æ ‡ï¼šäº¤æ˜“ Y_realized[t] (Day T çš„æ”¶ç›Š)
                # çº¦æŸï¼šåªèƒ½çœ‹ Day T-1 åŠä»¥å‰çš„æ•°æ®
                
                # ä½ çš„ infer_start_idx ä½¿å¾— X_infer_scaled çš„å¯¹é½å¦‚ä¸‹ï¼š
                # å‡è®¾ Lookback=60
                # å¦‚æœæˆ‘ä»¬ä» infer_start_idx = test_start - 60 å¼€å§‹å–
                # é‚£ä¹ˆ X_infer_scaled[59] æ˜¯ Day T-1
                # é‚£ä¹ˆ X_infer_scaled[60] æ˜¯ Day T
                
                # æ­£ç¡®çš„çª—å£ï¼š[t : t + 60] 
                # è¿™é‡Œ t=0 æ—¶ï¼Œå–çš„æ˜¯ [0:60]ï¼Œæœ€åä¸€ä¸ªç‚¹æ˜¯ index 59 (Day T-1)
                # è¿™æ ·å°±æ˜¯ï¼šç”¨è¿‡å» 60 å¤© (æˆªè‡³æ˜¨å¤©) çš„æ•°æ®ï¼Œé¢„æµ‹ä»Šå¤©çš„ä»“ä½
                
                x_window = X_infer_scaled[t : t + cfg.LOOKBACK_WINDOW]
                
                # ä¹‹å‰çš„é”™è¯¯ä»£ç æ˜¯å–äº† [t+1 : t+1+60] æˆ–è€…ç±»ä¼¼çš„åç§»å¯¼è‡´çœ‹åˆ°äº† Day T
                # åŠ¡å¿…ç¡®ä¿ä½ çš„ X_infer_scaled æ„é€ æ–¹å¼æ”¯æŒè¿™ç§åˆ‡ç‰‡
                
                # è®©æˆ‘ä»¬ç”¨æ›´ç›´è§‚çš„æ–¹å¼é‡æ–°åˆ‡ç‰‡ï¼Œé˜²æ­¢ç´¢å¼•æ··ä¹±ï¼š
                # æˆ‘ä»¬éœ€è¦ "æˆªæ­¢åˆ° t-1 çš„ Lookback ä¸ªæ•°æ®"
                # åœ¨ all_features ä¸­ï¼ŒDay T çš„ç´¢å¼•æ˜¯ test_start_idx + t
                # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ range: [test_start_idx + t - Lookback : test_start_idx + t]
                
                # è¿™ç§ç»å¯¹ç´¢å¼•æ³•æœ€å®‰å…¨ï¼Œä¸ä¼šé”™ï¼š
                curr_abs_idx = test_start_idx + t
                x_raw_window = all_features[curr_abs_idx - cfg.LOOKBACK_WINDOW : curr_abs_idx]
                
                # å®‰å…¨æ£€æŸ¥
                if len(x_raw_window) != cfg.LOOKBACK_WINDOW: 
                    # åªæœ‰å¹´åˆç¬¬ä¸€å¤©å¯èƒ½é‡åˆ°è¿™ä¸ªé—®é¢˜ï¼ˆå¦‚æœæ•°æ®ä¸å¤Ÿï¼‰ï¼Œé€šå¸¸ä¸ä¼š
                    # å¦‚æœä¸å¤Ÿï¼Œå°±è·³è¿‡æˆ–ç”¨ 1/N
                    results_dmpo.append(results_ew[-1] if results_ew else 0.0) 
                    results_ew.append(0.0)
                    continue

                # å®æ—¶å½’ä¸€åŒ– (ç”¨å½“å¹´çš„ scaler)
                x_window_scaled = scaler.transform(x_raw_window)
                # ===========================================
                
                x_tensor = torch.tensor(x_window_scaled).unsqueeze(0).to(device).double()
                
                # é¢„æµ‹
                w_pred, _, _ = model(x_tensor, curr_w)
                w_action = w_pred[0, 0, :] 
                
                # è®°å½•ç»“æœ
                w_np = w_action.cpu().numpy()
                y_today = Y_realized[t]
                
                # ... (åç»­è®¡ç®—æ”¶ç›Šé€»è¾‘ä¸å˜)
                w_prev_np = curr_w[0].cpu().numpy()
                turnover = np.sum(np.abs(w_np - w_prev_np))
                cost = turnover * cfg.COST_COEFF
                
                gross_ret = np.sum(w_np * y_today)
                net_ret = gross_ret - cost
                results_dmpo.append(net_ret)
                
                w_ew = np.ones(cfg.NUM_ASSETS) / cfg.NUM_ASSETS
                ret_ew = np.sum(w_ew * y_today)
                results_ew.append(ret_ew)
                
                curr_w = w_action.unsqueeze(0)
                
    # 3. ç»“æœæ±‡æ€»
    print("\nğŸ“Š è®¡ç®—æœ€ç»ˆæŒ‡æ ‡...")
    
    total_days = len(results_dmpo)
    idx = dates[-total_days:]
    
    s_dmpo = pd.Series(results_dmpo, index=idx)
    s_ew = pd.Series(results_ew, index=idx)
    
    # å‡€å€¼æ›²çº¿
    wealth_dmpo = (1 + s_dmpo).cumprod()
    wealth_ew = (1 + s_ew).cumprod()
    
    # æŒ‡æ ‡è®¡ç®—å‡½æ•°
    def calc_metrics(series, name):
        ann_ret = series.mean() * 252
        ann_vol = series.std() * np.sqrt(252)
        sharpe = (ann_ret - 0.02) / (ann_vol + 1e-6)
        
        downside = series[series<0]
        sortino = (ann_ret - 0.02) / (downside.std() * np.sqrt(252) + 1e-6)
        
        cum = (1+series).cumprod()
        dd = (cum - cum.cummax()) / cum.cummax()
        max_dd = dd.min()
        
        calmar = ann_ret / (abs(max_dd) + 1e-6)
        
        return {
            "Strategy": name,
            "Return": f"{ann_ret:.2%}",
            "Sharpe": f"{sharpe:.2f}",
            "Sortino": f"{sortino:.2f}",
            "Calmar": f"{calmar:.2f}",
            "MaxDD": f"{max_dd:.2%}"
        }
    
    m1 = calc_metrics(s_dmpo, "Diff-MPO (Factor Model)")
    m2 = calc_metrics(s_ew, "1/N Benchmark")
    
    res_df = pd.DataFrame([m1, m2])
    print("\nğŸ† æ»šåŠ¨å›æµ‹æœ€ç»ˆç»“æœ:")
    print(res_df)
    
    # ç”»å›¾
    plt.figure(figsize=(12, 6))
    plt.plot(wealth_dmpo, label='Diff-MPO (Factor Model)', linewidth=2)
    plt.plot(wealth_ew, label='1/N Benchmark', linestyle='--', alpha=0.7)
    plt.title(f'Walk-Forward: Factor Model vs 1/N ({TEST_START_YEAR}-{TEST_END_YEAR})', fontsize=14)
    plt.ylabel('Cumulative Wealth')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    save_path = 'walk_forward_factor_model.png'
    plt.savefig(save_path, dpi=300)
    print(f"\nğŸ“ˆ å›¾ç‰‡å·²ä¿å­˜è‡³: {save_path}")

if __name__ == "__main__":
    run_walk_forward_experiment()
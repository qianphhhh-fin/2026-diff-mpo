import torch
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

from config import cfg
from data_loader import load_and_process_data
from model import MPO_Network

# ä¿®æ”¹ train.py

def sharpe_loss(w_plan, y_future, w_prev, cost_coeff=0.01): # <--- ä¼ å…¥ w_prev å’Œ cost_coeff
    """
    w_plan: (Batch, Horizon, Assets)
    y_future: (Batch, Horizon, Assets)
    w_prev: (Batch, Assets)
    """
    # 1. è®¡ç®—æ¯›æ”¶ç›Š
    gross_ret = (w_plan * y_future).sum(dim=2) # (Batch, Horizon)
    
    # 2. è®¡ç®—äº¤æ˜“æˆæœ¬ (ä¸Ž Solver ä¿æŒä¸€è‡´çš„ L1 Norm)
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¡ç®— w_plan[t] - w_plan[t-1] çš„å®Œæ•´åºåˆ—
    # æž„é€ å®Œæ•´çš„æƒé‡è·¯å¾„: [w_prev, w_0, w_1, ..., w_{H-1}]
    # è¿™ä¸€æ­¥ç¨å¾®æœ‰ç‚¹ç¹çï¼Œä½†å¿…é¡»åš
    
    # å°† w_prev æ‰©å±•ä¸º (Batch, 1, Assets) ä»¥ä¾¿æ‹¼æŽ¥
    w_prev_expanded = w_prev.unsqueeze(1)
    
    # æ‹¼æŽ¥: (Batch, H+1, Assets)
    w_all = torch.cat([w_prev_expanded, w_plan], dim=1)
    
    # è®¡ç®—å·®åˆ†: |w_t - w_{t-1}|
    turnover = torch.norm(w_all[:, 1:] - w_all[:, :-1], p=1, dim=2) # (Batch, Horizon)
    
    # 3. è®¡ç®—å‡€æ”¶ç›Š (Net Return)
    net_ret = gross_ret - cost_coeff * turnover
    
    # 4. è®¡ç®— Sharpe (åŸºäºŽå‡€æ”¶ç›Š)
    mean_ret = net_ret.mean(dim=1)
    std_ret = net_ret.std(dim=1) + 1e-6
    sharpe = mean_ret / std_ret
    
    return -sharpe.mean()

# ==========================
# 2. è®­ç»ƒä¸»å¾ªçŽ¯
# ==========================
def train():
    # å‡†å¤‡æ•°æ®
    train_loader, test_loader, _ = load_and_process_data()
    
    # åˆå§‹åŒ–æ¨¡åž‹
    model = MPO_Network().to(cfg.DEVICE).double() # CVXPY éœ€è¦ Double ç²¾åº¦
    optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)
    
    print(f"ðŸš€ æ¨¡åž‹å·²åŠ è½½è‡³ {cfg.DEVICE}. å¼€å§‹è®­ç»ƒ {cfg.EPOCHS} Epochs...")
    
    loss_history = []
    
    for epoch in range(cfg.EPOCHS):
        model.train()
        epoch_loss = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{cfg.EPOCHS}")
        
        for batch_idx, (x, y) in enumerate(pbar):
            x, y = x.to(cfg.DEVICE).double(), y.to(cfg.DEVICE).double()
            
            # åˆå§‹æŒä»“ï¼šå‡è®¾æ¯ä¸ª Batch å¼€å§‹æ—¶æ˜¯ç©ºä»“æˆ–è€…å‡åŒ€æŒä»“
            # åœ¨çœŸå®žçš„ LSTM åºåˆ—è®­ç»ƒä¸­ï¼Œåº”è¯¥æŠŠä¸Šä¸€ä¸ª Batch çš„ w ä¼ è¿›æ¥
            # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œå‡è®¾æ¯å¤©æ—©ä¸Šéƒ½ä»Ž 1/N å¼€å§‹è°ƒä»“ (æˆ–è€…å…¨çŽ°é‡‘)
            # æ›´å¥½çš„åšæ³•æ˜¯: w_prev = torch.ones(...) / N
            w_prev = torch.ones(x.size(0), cfg.NUM_ASSETS, device=cfg.DEVICE, dtype=torch.double) / cfg.NUM_ASSETS
            
            # --- Forward ---
            # w_plan æ˜¯ Solver è§£å‡ºæ¥çš„æœ€ä¼˜è·¯å¾„
            w_plan, mu_pred, L_pred = model(x, w_prev)
            
            # --- Loss ---
            # ä½¿ç”¨æ–°çš„å¸¦æˆæœ¬çš„ Lossï¼Œä¼ å…¥ cfg.COST_COEFF
            loss = sharpe_loss(w_plan, y, w_prev, cost_coeff=cfg.COST_COEFF)
            
            # --- Backward ---
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª (é˜²æ­¢ LSTM æ¢¯åº¦çˆ†ç‚¸)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix({'SharpeLoss': f"{loss.item():.4f}"})
        
        avg_loss = epoch_loss / len(train_loader)
        loss_history.append(avg_loss)
        print(f"Epoch {epoch+1} Average Loss: {avg_loss:.4f} (Implied Sharpe: {-avg_loss:.4f})")
        
    # ==========================
    # 3. ç®€å•çš„ç»“æžœå¯è§†åŒ–
    # ==========================
    plt.figure(figsize=(10, 5))
    plt.plot(loss_history, label='Negative Sharpe Loss')
    plt.title('Training Progress')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('diff_mpo_training_loss.png')
    print("ðŸ“ˆ è®­ç»ƒå®Œæˆï¼ŒLoss æ›²çº¿å·²ä¿å­˜è‡³ training_loss.png")
    
    # ä¿å­˜æ¨¡åž‹

    SAVE_PATH = 'models/diff_mpo_sharpe.pth'  # ç§‘å­¦å‘½å
    torch.save(model.state_dict(), SAVE_PATH)
    print(f"ðŸ† Diff-MPO (Ours) æ¨¡åž‹å·²ä¿å­˜è‡³: {SAVE_PATH}")


if __name__ == "__main__":
    train()
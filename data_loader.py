import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from config import cfg  # å¯¼å…¥é…ç½®

class MPODataset(Dataset):
    """
    é’ˆå¯¹å¤šæœŸä¼˜åŒ–çš„æ•°æ®é›†ç±»ã€‚
    è¾“å…¥ X: è¿‡å» T å¤©çš„æ‰€æœ‰ç‰¹å¾ (å®è§‚ + å› å­ + æŠ€æœ¯é¢)
    è¾“å‡º Y: æœªæ¥ H å¤©çš„ã€èµ„äº§çœŸå®æ”¶ç›Šç‡ã€‘ (ç”¨äºè®¡ç®— Loss: Sharpe Ratio)
    """
    def __init__(self, features, returns, lookback=60, horizon=5):
        """
        features: (N, F) å½’ä¸€åŒ–åçš„ç‰¹å¾çŸ©é˜µ
        returns:  (N, A) åŸå§‹èµ„äº§æ”¶ç›Šç‡çŸ©é˜µ (ä¸å½’ä¸€åŒ–ï¼ç®—é’±å¿…é¡»ç”¨çœŸå€¼)
        """
        self.features = torch.FloatTensor(features)
        self.returns = torch.FloatTensor(returns)
        self.lookback = lookback
        self.horizon = horizon
        
        # æœ‰æ•ˆæ ·æœ¬æ•° = æ€»é•¿åº¦ - å›çœ‹çª—å£ - é¢„æµ‹çª—å£
        self.length = len(features) - lookback - horizon + 1

    def __len__(self):
        return max(0, self.length)

    def __getitem__(self, idx):
        # 1. è¾“å…¥ X: ä» idx åˆ° idx+lookback (è¿‡å»Tå¤©)
        # å½¢çŠ¶: (T, Num_Features)
        x_window = self.features[idx : idx + self.lookback]
        
        # 2. æ ‡ç­¾ Y: ä» idx+lookback åˆ° idx+lookback+horizon (æœªæ¥Hå¤©)
        # å½¢çŠ¶: (H, Num_Assets)
        # æ³¨æ„ï¼šè¿™æ˜¯ Solver ä¼˜åŒ–å®Œä¹‹åï¼Œç”¨æ¥â€œå¯¹ç­”æ¡ˆâ€çš„çœŸå®æœªæ¥æ”¶ç›Š
        y_horizon = self.returns[idx + self.lookback : idx + self.lookback + self.horizon]
        
        return x_window, y_horizon

def load_and_process_data():
    """
    ä¸»å‡½æ•°ï¼šè¯»å–CSV -> æ¸…æ´— -> æ‹†åˆ† -> å½’ä¸€åŒ– -> æ„å»ºDataLoader
    """
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®: {cfg.DATA_PATH} ...")
    df = pd.read_csv(cfg.DATA_PATH, index_col=0, parse_dates=True)
    
    # 1. æ‹†åˆ†ç‰¹å¾ä¸ç›®æ ‡
    # Target: åªæœ‰é‚£5ä¸ªæˆ‘ä»¬è¦äº¤æ˜“çš„èµ„äº§
    asset_returns = df[cfg.ASSETS].values
    
    # Features: åŒ…å«å®è§‚ã€æŠ€æœ¯é¢ã€ä»¥åŠèµ„äº§è‡ªèº«çš„æ»åæ”¶ç›Š
    # (åœ¨ fetch_data ä¸­æˆ‘ä»¬å·²ç»æŠŠæ‰€æœ‰åˆ—éƒ½æ‹¼åœ¨äº†ä¸€èµ·ï¼Œè¿™é‡Œç›´æ¥ç”¨å…¨éƒ¨åˆ—ä½œä¸ºç‰¹å¾)
    # æ³¨æ„ï¼šé€šå¸¸ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¯ä»¥åªé€‰éƒ¨åˆ†åˆ—ï¼Œè¿™é‡Œå…ˆå…¨ç”¨
    feature_data = df.values
    
    # 2. åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›† (æŒ‰æ—¶é—´åˆ‡åˆ†ï¼Œä¸¥ç¦ Shuffle!)
    split_date = pd.Timestamp(cfg.TRAIN_SPLIT_DATE)
    train_mask = df.index < split_date
    test_mask = df.index >= split_date
    
    print(f"   è®­ç»ƒé›†æˆªæ­¢: {cfg.TRAIN_SPLIT_DATE} (æ ·æœ¬æ•°: {sum(train_mask)})")
    print(f"   æµ‹è¯•é›†å¼€å§‹: {cfg.TRAIN_SPLIT_DATE} (æ ·æœ¬æ•°: {sum(test_mask)})")
    
    X_train_raw = feature_data[train_mask]
    X_test_raw = feature_data[test_mask]
    
    # Y ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œå› ä¸ºç”±äºæˆ‘ä»¬è¦ç®—çœŸå®çš„å¤æ™®æ¯”ç‡
    Y_train = asset_returns[train_mask]
    Y_test = asset_returns[test_mask]
    
    # 3. å½’ä¸€åŒ– (Z-Score)
    # å…³é”®ï¼šScaler åªèƒ½åœ¨è®­ç»ƒé›†ä¸Š fitï¼Œç„¶å transform åˆ°æµ‹è¯•é›†ï¼
    # å¦åˆ™å°±æ˜¯ä¸¥é‡çš„ Look-ahead Bias (æ•°æ®æ³„éœ²)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_raw)
    X_test_scaled = scaler.transform(X_test_raw) # ç”¨è®­ç»ƒé›†çš„å‡å€¼æ–¹å·®å¤„ç†æµ‹è¯•é›†
    
    print("âœ… æ•°æ®å½’ä¸€åŒ–å®Œæˆ (StandardScaler)")
    
    # 4. æ„å»º Dataset
    train_dataset = MPODataset(
        X_train_scaled, Y_train, 
        lookback=cfg.LOOKBACK_WINDOW, 
        horizon=cfg.PREDICT_HORIZON
    )
    
    test_dataset = MPODataset(
        X_test_scaled, Y_test, 
        lookback=cfg.LOOKBACK_WINDOW, 
        horizon=cfg.PREDICT_HORIZON
    )
    
    # 5. æ„å»º DataLoader
    # è®­ç»ƒé›†å¯ä»¥ shuffleï¼Œå¢åŠ æ³›åŒ–èƒ½åŠ›
    # æµ‹è¯•é›†ä¸è¦ shuffleï¼Œæ–¹ä¾¿ç”»å‡ºè¿ç»­çš„èµ„é‡‘æ›²çº¿
    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, drop_last=True)
    
    return train_loader, test_loader, scaler

# ==========================
# å•å…ƒæµ‹è¯• (Run this file directly)
# ==========================
if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹è¿è¡Œæ•°æ®åŠ è½½æµ‹è¯•...")
    try:
        train_loader, test_loader, _ = load_and_process_data()
        
        # å–å‡ºä¸€ä¸ª Batch çœ‹çœ‹å½¢çŠ¶
        x_batch, y_batch = next(iter(train_loader))
        
        print(f"\n[æµ‹è¯•é€šè¿‡] Batch Shapes:")
        print(f"   X (Input Features): {x_batch.shape}")
        print(f"     -> (Batch={cfg.BATCH_SIZE}, Lookback={cfg.LOOKBACK_WINDOW}, Feats={x_batch.shape[2]})")
        print(f"   Y (Future Returns): {y_batch.shape}")
        print(f"     -> (Batch={cfg.BATCH_SIZE}, Horizon={cfg.PREDICT_HORIZON}, Assets={cfg.NUM_ASSETS})")
        
        print("\nğŸš€ data_loader æ¨¡å—å·¥ä½œæ­£å¸¸ï¼è¯·ç»§ç»­ä¸‹ä¸€æ­¥ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from tqdm import tqdm
from config import cfg
from data_loader import load_and_process_data
from model import MPO_Network
from mpo_solver import DifferentiableMPO # ç¡®ä¿èƒ½å¯¼å…¥

# è®¾ç½®è®¾å¤‡
DEVICE = cfg.DEVICE
plt.style.use('seaborn-v0_8')

def run_neuron_analysis():
    print("ğŸ§  æ­£åœ¨è¿›è¡Œ Diff-MPO ç¥ç»å…ƒé˜»æ–­åˆ†æ (Neuron Ablation)...")
    
    # 1. å‡†å¤‡æ•°æ®
    _, test_loader, _ = load_and_process_data()
    df = pd.read_csv(cfg.DATA_PATH, index_col=0, parse_dates=True)
    split_date = pd.Timestamp(cfg.TRAIN_SPLIT_DATE)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®å¯¹åº” test_loader
    # test_loader çš„é•¿åº¦å¯èƒ½å› ä¸º batch drop last è€Œç•¥çŸ­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹é½
    test_dates = df.index[df.index >= split_date]
    
    # è·å–æµ‹è¯•é›†æœŸé—´çš„å¸‚åœºåŸºå‡† (ç”¨äºå¯¹æ¯”ç”»å›¾)
    market_prices = df.loc[test_dates, cfg.ASSETS[0]] 
    market_cum_ret = (1 + market_prices.pct_change().fillna(0)).cumprod()

    # 2. åŠ è½½æ¨¡å‹
    model = MPO_Network().to(DEVICE).double()
    try:
        model.load_state_dict(torch.load('models/diff_mpo_sharpe.pth', map_location=DEVICE))
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ models/diff_mpo_sharpe.pthï¼Œè¯·å…ˆè¿è¡Œ eval.py æˆ– train.py")
        return

    model.eval()
    
    # è·å–éšè—å±‚ç»´åº¦
    hidden_dim = model.lstm.hidden_size
    print(f"   æ£€æµ‹åˆ° LSTM éšè—å±‚ç»´åº¦: {hidden_dim}")

    # åˆå§‹åŒ– Solver (åœ¨ CPU ä¸Šè¿è¡Œä»¥é¿å… cvxpylayers çš„ CUDA bug)
    # æ³¨æ„ï¼šä¸è¦æŠŠè¿™ä¸ª solver æ”¾åˆ° GPU ä¸Šï¼Œå› ä¸ºæˆ‘ä»¬è¦å–‚ç»™å®ƒ CPU æ•°æ®
    solver = DifferentiableMPO()

    # ==========================================
    # 3. å®šä¹‰è¯„ä¼°æ ¸å¿ƒ (æ”¯æŒç¥ç»å…ƒé˜»æ–­)
    # ==========================================
    def get_performance(mask_neuron_idx=None):
        """
        mask_neuron_idx: int, è¦é˜»æ–­çš„ç¥ç»å…ƒç´¢å¼• (0 ~ 63)
        è¿”å›: Sharpe Ratio
        """
        all_net_ret = []
        # current_w æ”¾åœ¨ CPU ä¸Šï¼Œæ–¹ä¾¿åç»­ç›´æ¥å¤„ç†
        current_w = torch.ones(cfg.NUM_ASSETS, dtype=torch.double) / cfg.NUM_ASSETS
        
        # éå†æµ‹è¯•é›†
        for x_batch, y_batch in test_loader:
            # è¾“å…¥æ•°æ®ä¾ç„¶å» GPU (ä¸ºäº† LSTM æ¨ç†é€Ÿåº¦)
            x_batch = x_batch.to(DEVICE).double()
            # y_batch ä¹Ÿå» GPUï¼Œä½†ç¨åè®¡ç®—æ”¶ç›Šæ—¶æˆ‘ä»¬ä¼šæ‹‰å› CPU
            y_batch = y_batch.to(DEVICE).double()
            
            batch_size = x_batch.size(0)
            
            with torch.no_grad():
                # 1. LSTM Forward (GPU)
                _, (h_n, _) = model.lstm(x_batch)
                context = h_n[-1] # (Batch, Hidden)
                
                # --- ğŸ§  å…³é”®æ‰‹æœ¯ï¼šç¥ç»å…ƒé˜»æ–­ ---
                if mask_neuron_idx is not None:
                    context[:, mask_neuron_idx] = 0.0
                
                # 2. Heads (GPU)
                mu_pred = model.mu_head(context).view(batch_size, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS)
                L_flat = model.L_head(context)
                L_pred = L_flat.view(batch_size, cfg.PREDICT_HORIZON, cfg.NUM_ASSETS, cfg.NUM_ASSETS)
                
                # æ„é€  L (GPU)
                mask = torch.tril(torch.ones_like(L_pred))
                L_pred = L_pred * mask
                diag_mask = torch.eye(cfg.NUM_ASSETS, device=DEVICE).view(1, 1, cfg.NUM_ASSETS, cfg.NUM_ASSETS)
                L_pred = L_pred + diag_mask * (torch.nn.functional.softplus(L_pred) + 1e-5 - L_pred)
                
                # 3. Solver (CPU)
                # âš ï¸ã€ä¿®å¤å…³é”®ã€‘âš ï¸ï¼šå°†æ‰€æœ‰ Tensor ç§»å› CPU å†ä¼ ç»™ Solver
                # cvxpylayers åœ¨å¤„ç† CUDA tensor è½¬ numpy æ—¶ç»å¸¸æŠ¥é”™
                mu_cpu = mu_pred.cpu()
                L_cpu = L_pred.cpu()
                w_prev_cpu = current_w.repeat(batch_size, 1) # current_w å·²ç»åœ¨ CPU äº†
                
                # è°ƒç”¨ Solver (åœ¨ CPU ä¸Šè§£)
                w_plan = solver(mu_cpu, L_cpu, w_prev_cpu)
                
                # 4. è®¡ç®—æ”¶ç›Š (CPU)
                w_t = w_plan[:, 0, :] # (Batch, Assets)
                y_t = y_batch[:, 0, :].cpu() # çœŸå®æ”¶ç›Šç§»å› CPU
                
                # è®¡ç®— Gross Return
                ret = (w_t * y_t).sum(dim=1)
                all_net_ret.append(ret.numpy())
                
        all_net_ret = np.concatenate(all_net_ret)
        mean = np.mean(all_net_ret) * 252
        std = np.std(all_net_ret) * np.sqrt(252)
        return mean / (std + 1e-6)

    # ==========================================
    # 4. æ‰§è¡Œåˆ†æ
    # ==========================================
    
    # A. è®¡ç®— Baseline
    print("   è®¡ç®— Baseline æ€§èƒ½...")
    base_sharpe = get_performance(mask_neuron_idx=None)
    print(f"   âœ… Baseline Sharpe: {base_sharpe:.4f}")
    
    # B. éå†æ‰€æœ‰ç¥ç»å…ƒ
    importance = []
    print(f"   æ­£åœ¨æ‰«æ {hidden_dim} ä¸ªç¥ç»å…ƒ...")
    
    for i in tqdm(range(hidden_dim)):
        s = get_performance(mask_neuron_idx=i)
        # Drop > 0 è¡¨ç¤ºè¯¥ç¥ç»å…ƒå¯¹æ­£å‘æ”¶ç›Šæœ‰è´¡çŒ®ï¼ˆé˜»æ–­å®ƒå¯¼è‡´ Sharpe ä¸‹é™ï¼‰
        drop = (base_sharpe - s) / (abs(base_sharpe) + 1e-6)
        importance.append(drop)
    
    importance = np.array(importance)
    
    # C. æ‰¾åˆ° Top-K ç¥ç»å…ƒ
    top_k_indices = np.argsort(importance)[::-1][:5] # å–ä¸‹é™å¹…åº¦æœ€å¤§çš„å‰5ä¸ª
    print("\nğŸ† æœ€é‡è¦çš„åŠŸèƒ½æ€§ç¥ç»å…ƒ (Top 5):")
    for idx in top_k_indices:
        print(f"   Neuron #{idx}: Sharpe ä¸‹é™ {importance[idx]:.2%}")
        
    top_neuron_idx = top_k_indices[0]
    
    # ==========================================
    # 5. å¯è§†åŒ– Top ç¥ç»å…ƒçš„æ¿€æ´»è¡Œä¸º
    # ==========================================
    print(f"\nğŸ“¸ æ­£åœ¨ç»˜åˆ¶ Neuron #{top_neuron_idx} çš„æ—¶åºæ¿€æ´»å›¾...")
    
    activations = []
    # å†æ¬¡éå†æå–æ¿€æ´»å€¼
    for x_batch, _ in test_loader:
        x_batch = x_batch.to(DEVICE).double()
        with torch.no_grad():
            _, (h_n, _) = model.lstm(x_batch)
            act = h_n[-1][:, top_neuron_idx].cpu().numpy()
            activations.append(act)
            
    activations = np.concatenate(activations)
    
    # å¯¹é½é•¿åº¦
    plot_len = min(len(activations), len(test_dates))
    dates = test_dates[:plot_len]
    mkt_curve = market_cum_ret.iloc[:plot_len]
    act_data = activations[:plot_len]
    
    # ç»˜å›¾
    fig, ax1 = plt.subplots(figsize=(12, 6))
    
    # ç»˜åˆ¶å¸‚åœºæ›²çº¿
    color = 'tab:gray'
    ax1.set_xlabel('Date')
    ax1.set_ylabel('Market Cumulative Return', color=color)
    ax1.plot(dates, mkt_curve, color=color, alpha=0.5, label='Market (Benchmark)', linestyle='--')
    ax1.tick_params(axis='y', labelcolor=color)
    
    # ç»˜åˆ¶ç¥ç»å…ƒæ¿€æ´»å€¼
    ax2 = ax1.twinx()  
    color = 'tab:red'
    ax2.set_ylabel(f'Neuron #{top_neuron_idx} Activation', color=color, fontsize=12, fontweight='bold')
    # ä½¿ç”¨æ•£ç‚¹å›¾æˆ–ç»†çº¿ï¼Œå› ä¸ºæ¿€æ´»å€¼æ³¢åŠ¨å¯èƒ½å¾ˆå¿«
    ax2.plot(dates, act_data, color=color, alpha=0.8, linewidth=1.0, label=f'Neuron #{top_neuron_idx}')
    ax2.tick_params(axis='y', labelcolor=color)
    
    plt.title(f'Mechanistic Analysis: What does Neuron #{top_neuron_idx} do?', fontsize=14)
    fig.autofmt_xdate()
    plt.tight_layout()
    
    save_path = 'neuron_analysis.png'
    plt.savefig(save_path, dpi=100)
    print(f"âœ… ç»“æœå·²ä¿å­˜è‡³ {save_path}")
    
    # ä¿å­˜ CSV
    df_imp = pd.DataFrame({'Neuron_ID': range(hidden_dim), 'Importance_Drop': importance})
    df_imp.to_csv('neuron_importance.csv', index=False)

if __name__ == "__main__":
    run_neuron_analysis()